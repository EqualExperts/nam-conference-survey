# Interview: Katie Coleman

**Date:** 2025-11-12 (14:30)
**Interviewer:** Mike Mitchell
**Persona:** Conference data consumer and decision maker

## Participant Background

**Role**: Managing Director at Equal Experts

**Experience**: Led conference feedback collection using Google Forms; responsible for strategic decisions about future conference investments

**Company/Context**: Equal Experts software consultancy manages network engagement initiatives including annual conferences with significant budgets. Oversees 40-person conference scheduled on Saturday, requiring personal time commitment from attendees working primarily for external customers.

## Pain Points and Problem Areas

### 1. Saturday Personal Time Concern
- **Quote**: "We're very conscious that this is on a Saturday, and it's people's personal time that we're routing into"
- **Context**: Weekend conference scheduling creates personal sacrifice expectations
- **Frequency**: Annual concern
- **Impact**:
  - Severity: High
  - Consequences: Risk of poor attendance, negative sentiment, value proposition questioning
  - Emotional impact: Conscious guilt about requesting personal time
- **Current Workaround**: Maximizing value through networking and learning opportunities
- **Underlying Need**: Justify personal time investment with clear value delivery

### 2. Uncertain ROI on Conference Investment
- **Quote**: "These are expensive things. Do they still create the value or should we do something different next time?"
- **Context**: Strategic decision-making about future conference investments lacks clear value measurement
- **Frequency**: Annual strategic decision
- **Impact**:
  - Severity: High
  - Consequences: Risk of continuing ineffective programs or cancelling valuable ones
  - Emotional impact: Uncertainty about resource allocation
- **Current Workaround**: Relying on general positive feedback without detailed analysis
- **Underlying Need**: Measure concrete value to justify continued investment

### 3. Timing Challenges for Follow-up
- **Quote**: "Once people leave, back to their lives, back to work, it's another thing to do, isn't it?"
- **Context**: Getting feedback after attendees return to normal routines
- **Frequency**: Every feedback collection cycle
- **Impact**:
  - Severity: Medium
  - Consequences: Lower response rates, less accurate sentiment
  - Emotional impact: Frustration with missed opportunities
- **Current Workaround**: Sending follow-up reminders
- **Underlying Need**: Capture feedback while experience is fresh and top-of-mind

### 4. Limited Post-Conference Action
- **Quote**: "We didn't really know what to do afterwards. Like, there was a bunch of ideas and thoughts. We captured a lot shared it, but trying to teach something like consulting skills… feels impossible"
- **Context**: Previous year's consulting skills conference lacked actionable outcomes
- **Frequency**: Post-conference follow-through challenge
- **Impact**:
  - Severity: Medium
  - Consequences: Attendees perceive conference as one-off event without lasting impact
  - Emotional impact: Sense that investment didn't create lasting change
- **Current Workaround**: Accepting that some topics are inherently difficult to follow up
- **Underlying Need**: Create sustainable learning and improvement beyond the event

## Unmet Needs

### 1. Real-time Feedback Capture
- **Need Statement**: Collect feedback while attendees remain on-site before departure
- **Context**: Maximizing response rates and accuracy of sentiment
- **Type of Need**: Timing constraint/requirement
- **Associated with Pain?**: Yes - addresses timing and response rate challenges
- **Priority**: High
- **Success Criteria**: Capture feedback "then and there before people leave"

### 2. Multi-dimensional Feedback Categories
- **Need Statement**: Capture sentiment, logistics, learning, and networking feedback separately
- **Context**: Four distinct areas requiring separate measurement—emotional sentiment, practical logistics, learning outcomes, networking effectiveness
- **Type of Need**: Quality requirement for comprehensive feedback
- **Associated with Pain?**: No - enables comprehensive understanding
- **Priority**: Must-have for complete picture
- **Success Criteria**: Clear insights across all four dimensions

### 3. Connection to Equal Experts Community
- **Need Statement**: Measure how conference affects attendees' connection to Equal Experts
- **Context**: Remote consultants working primarily for customers need stronger company connection
- **Type of Need**: Emotional/community need
- **Associated with Pain?**: No - builds community beyond problem-solving
- **Priority**: High for retention and engagement
- **Success Criteria**: Measurable improvement in company connection sentiment

### 4. Public Transparency
- **Need Statement**: Share feedback results publicly on blog to demonstrate value
- **Context**: Justifying significant financial investment to stakeholders
- **Type of Need**: Accountability/transparency requirement
- **Associated with Pain?**: No - meeting organizational values
- **Priority**: Nice-to-have but organizationally important
- **Success Criteria**: Publishable summary of conference impact

### 5. Rapid Follow-up on Learning Opportunities
- **Need Statement**: Quick response to AI learning continuation requests within one month
- **Context**: Timely follow-up when attendees want to continue AI/SDLC conversations
- **Type of Need**: Timing constraint for learning engagement
- **Associated with Pain?**: No - enabling continued engagement
- **Priority**: Medium for learning outcomes
- **Success Criteria**: Follow-up within "a month or so" for continued learning

## Aspirations & Opportunities

### 1. Conversational AI Feedback Collection
- **Aspiration**: "What would be really interesting… could you have a conversational LLMs like an agent that's interviewing you so you'll effectively just speaking to it"
- **Inspiration Source**: ChatGPT conversational surveys experience; John's question-based interaction
- **Why It Excites Them**: Could reimagine entire feedback experience beyond form constraints; more natural interaction
- **Uncertainty/Questions**: Whether it's too complicated; how to balance with statistical data needs
- **Exploration vs Commitment**: Exploring/curious - "I don't know if we could do this"
- **Quote**: "Maybe we use AI as the tool that's helping to capture a lot of this… you just reimagine this whole way you get feedback"

### 2. Mobile-First Experience Design
- **Aspiration**: Target mobile devices for better in-the-moment capture
- **Inspiration Source**: Recognition that people might complete surveys on phones while at venue
- **Why It Excites Them**: Could increase response rates by meeting users where they are
- **Uncertainty/Questions**: "I don't know which device they would do it on… is it better to target a mobile device because you might catch them then and there?"
- **Exploration vs Commitment**: Interested and wanting to understand better

### 3. AI-Powered Analysis and Distillation
- **Aspiration**: Use AI to help distill qualitative feedback from conversational interactions
- **Inspiration Source**: Small group size (40 people) makes detailed analysis feasible
- **Why It Excites Them**: Could get richer insights without statistical analysis burden
- **Uncertainty/Questions**: How to balance qualitative richness with actionable insights
- **Exploration vs Commitment**: Exploring as part of broader AI feedback vision

## Current Workflow

### Conference Feedback Collection and Analysis

**Trigger**: Conference completion; need to measure success and plan future events

**Goal**: Gather comprehensive feedback to inform future conference decisions and demonstrate ROI

**Steps**:

1. **Create feedback form**
   - Actions: Use Google Forms to build survey
   - Tools/systems: Google Forms
   - Decision points: Choose question types (multiple choice vs descriptive)
   - Pain points: Limited to traditional form constraints

2. **Distribute survey to attendees**
   - Actions: Send survey link after conference
   - Blockers/friction: Timing challenge as people return to normal routines
   - Workarounds: Follow-up reminders to increase response

3. **Collect responses over 1-2 weeks**
   - Time required: "Most responses within that time frame"
   - Pain points: Declining response rates as time passes

4. **Analyze results**
   - Actions: Review feedback, identify patterns
   - Tools/systems: Google Forms analysis
   - Collaboration: Share with Lauren, Danielle, and team

5. **Document findings**
   - Actions: Write up results for internal and external sharing
   - Tools/systems: Blog publication
   - Completion Criteria: Published summary of conference impact

6. **Apply insights to next year's planning**
   - Actions: Use feedback to inform venue, format, schedule decisions
   - Decision points: Whether to continue conferences; what changes to make
   - Pain points: Difficulty translating feedback into actionable improvements

**Variations**: Previous year focused on consulting skills vs this year's AI focus affects follow-up strategies

**What's Missing**: Real-time capture capability, conversational interaction option, automated analysis

## Feature Reactions

### 1. Google Forms Experience
- **Initial Reaction**: Positive - "seemed to work pretty well"
- **Would they use it?**: Yes, used successfully before
- **Rationale**: "Easy to produce the simple UI to create the questions. And pretty easy to pick and choose what you wanted… what type of question you wanted"
- **Suggested Improvements**: No major improvements mentioned

### 2. Mobile/Device Targeting
- **Initial Reaction**: Curious and uncertain
- **Would they use it?**: Maybe—interested in exploring
- **Rationale**: Could improve response rates by catching people "then and there"
- **Suggested Improvements**: Need to understand optimal device targeting strategy

### 3. AI Conversational Interface
- **Initial Reaction**: Very interested - "what would be really interesting"
- **Would they use it?**: Exploring possibility
- **Rationale**: Could reimagine feedback beyond form constraints; more natural interaction
- **Additional Context**: Inspired by ChatGPT survey experience but uncertain about complexity trade-offs

## Memorable Quotes

> "We're very conscious that this is on a Saturday, and it's people's personal time that we're routing into in a way that we don't normally. So we want to get that sentiment."

> "These are expensive things. Do they still create the value or should we do something different next time?"

> "What would be really interesting… could you have a conversational LLMs like an agent that's interviewing you so you'll effectively just speaking to it"

> "Once people leave, back to their lives, back to work, it's another thing to do, isn't it?"

> "Maybe we use AI as the tool that's helping to capture a lot of this… you just reimagine this whole way you get feedback"

> "We didn't really know what to do afterwards. Like, there was a bunch of ideas and thoughts. We captured a lot shared it, but trying to teach something like consulting skills… feels impossible"

> "When you look at the feedback… what do people get out of it? Overwhelmingly. It's nearly always about the connections and meeting people"

## Behavioral Observations

- **Strategic cost-consciousness**: Repeatedly returns to ROI and value justification; views conference as significant financial investment requiring worth demonstration

- **Community-building focus**: Prioritizes networking and connection-building over content delivery; understands relationships drive value

- **Pragmatic about constraints**: Realistic about Saturday timing challenges and post-conference follow-up difficulties while seeking solutions

- **Open to innovation**: Genuinely excited about AI possibilities despite uncertainty about implementation complexity

- **Transparency-oriented**: Values sharing results publicly as organizational accountability and learning

- **Experience-informed**: Draws on previous conference learnings to shape current requirements and expectations

- **People-centered approach**: Focuses on attendee experience and emotional responses rather than logistical metrics alone

## Follow-up Questions

1. What was the specific response rate percentage from last year's conference feedback, and how did it compare to target?

2. Can you quantify the "expensive" investment—what does the conference cost relative to annual budget?

3. What specific logistics questions from last year revealed the biggest attendee pain points?

4. How do you currently measure "connection to Equal Experts"—do you have baseline metrics?

5. What percentage of attendees typically request follow-up learning opportunities, and how are those handled?

6. What specific actions resulted from last year's feedback, and which items went unaddressed?

7. How do networking expectations differ between UK conference attendees and this smaller group?

8. What would constitute "good enough" response rate for decision-making—75%, 80%, 90%?

9. Are there sensitive topics or feedback areas you avoid asking about in surveys?

10. How do Lauren and Danielle typically use the feedback data differently than you do?

## Next Steps

- **Katie Coleman**: Send previous conference feedback write-up and summary document to Mike for additional context
- **Mike Mitchell**: Interview remaining stakeholders including Lauren for network engagement perspective
- **Mike Mitchell**: Interview attendees Andrew and Chris Condol for user perspective
- **Mike Mitchell**: Consider AI conversational interface as potential feature option for development
