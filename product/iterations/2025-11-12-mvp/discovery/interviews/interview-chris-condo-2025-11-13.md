# Interview: Chris Condo

**Date:** 2025-11-13 (09:15)
**Interviewer:** Mike Mitchell
**Persona:** Client Partner at Equal Experts (survey taker and data recipient as part of NAM leadership team)

## Participant Background

**Role**: Client Partner at Equal Experts, North America

**Experience**: Former Forrester analyst with extensive experience in survey design and data analysis for thought leadership papers, total economic impact studies, and technology evaluations (waves). Used manual data researchers for question curation and third-party companies for anonymous survey distribution.

**Company/Context**: Equal Experts is an AI-first software consultancy focused on senior consultants helping clients transform through collaboration. Chris works with clients to deliver more value and identify opportunities for expanded engagement.

## Pain Points and Problem Areas

### 1. Survey Questions Miss the Mark
- **Quote**: "Sometimes the questions that you asked or the options that you gave didn't drive the results that you actually wanted. Like you got interesting information, but, like, that's really not answering the question. That is the crux of the problem."
- **Context**: During group think sessions designing surveys at Forrester
- **Frequency**: Recurring issue with survey design
- **Severity**: High
- **Consequences**: Wasted time and resources, inability to draw meaningful conclusions
- **Emotional Impact**: Frustration with getting interesting but irrelevant data
- **Current Workaround**: Working backwards from intended survey use to construct questions
- **What They're Really Trying to Do**: Design surveys that generate actionable insights for specific business decisions

### 2. Survey Timing Conflicts with Travel
- **Quote**: "A lot of times when you're traveling to a conference, you have to travel back. And so the survey comes at an inconvenient time. When you're traveling. And then you get home and you forget to take it."
- **Context**: Conference surveys sent after events
- **Frequency**: Common issue with post-conference surveys
- **Severity**: Medium
- **Consequences**: Lost responses, diminished emotional connection to experience
- **Emotional Impact**: Surveys get "lost in my inbox"
- **Current Workaround**: None effective - people simply miss taking surveys
- **What They're Really Trying to Do**: Capture feedback while experience is fresh and convenient

### 3. Loss of Raw Emotion Over Time
- **Quote**: "When you finally do take it, you don't necessarily have the same raw emotion that you had in the moment."
- **Context**: Delayed survey completion after conferences
- **Frequency**: Every time surveys are taken days/weeks later
- **Severity**: Medium
- **Consequences**: Less authentic, emotionally-driven feedback
- **Emotional Impact**: Disconnection from original experience
- **Current Workaround**: Waiting a few days for retrospective thinking, but this loses immediacy
- **What They're Really Trying to Do**: Capture authentic, in-the-moment feedback

### 4. Untested Survey Logic Failures
- **Quote**: "She didn't test the survey, and it turned out two of the questions never showed up… I'm like, where are the answers on these two questions? She's like, oh, I don't know."
- **Context**: Critical Forrester wave evaluation where months of preparation were involved
- **Frequency**: One-time but significant incident
- **Severity**: High
- **Consequences**: Missing critical data from expensive, time-intensive research
- **Emotional Impact**: "Not good. Not good."
- **Current Workaround**: Now emphasizes testing surveys before deployment
- **What They're Really Trying to Do**: Ensure all survey questions function properly before launch

### 5. Mishmash Survey Design by Committee
- **Quote**: "Every analyst. Would have. Their chance to say, I want this on the survey. And what would happen is that it would end up becoming kind of a mishmash. Of different technologies."
- **Context**: Team-based survey creation at Forrester
- **Frequency**: Standard practice for team surveys
- **Severity**: Medium
- **Consequences**: Respondents answering questions outside their expertise, watered-down results
- **Emotional Impact**: Reduced confidence in data quality
- **Current Workaround**: Focus surveys within specific scope and use branching logic
- **What They're Really Trying to Do**: Get knowledgeable responses from qualified respondents

### 6. Survey Perceived as Useless Management Exercise
- **Quote**: "If I think management's asking me to do a survey to make them feel better about themselves, I might not want to do it."
- **Context**: Employee surveys that don't lead to action
- **Frequency**: Common with corporate surveys
- **Severity**: High
- **Consequences**: Low response rates, disengaged participants
- **Emotional Impact**: Cynicism and reluctance to participate
- **Current Workaround**: Skip surveys perceived as meaningless
- **What They're Really Trying to Do**: Participate in surveys that drive real change

## Unmet Needs

### 1. Statistical Significance Standards
- **Need Statement**: Minimum 50 responses per question for statistically significant results
- **Context**: Forrester's standard for reliable survey data
- **Type of Need**: Quality requirement
- **Associated with Pain?**: No - enabling capability for credible research
- **Priority**: Must-have for formal research (not applicable for 40-person conference)

### 2. Raw Data Access Plus Interpreted Analysis
- **Need Statement**: "There's two ways I wanted… the raw excel of all the questions, all the answers, all the numbers… I want to create some very interesting slides based on each question"
- **Context**: As an analyst, needs both granular data and meaningful interpretation
- **Type of Need**: Enabling capability
- **Associated with Pain?**: No - requirement for thorough analysis
- **Success Criteria**: Excel export plus slide deck with trends and insights

### 3. Survey Strategy Before Question Design
- **Need Statement**: "Having a strategy for the survey is the most important thing"
- **Context**: Understanding survey purpose before creating questions
- **Type of Need**: Quality requirement
- **Associated with Pain?**: Yes - prevents misaligned questions
- **Priority**: Must-have
- **Success Criteria**: Clear understanding of how results will be used

### 4. 75% Response Rate Target
- **Need Statement**: "It would be great if you could get at least 30 people to fill it out" (out of 40 attendees)
- **Context**: Conference survey effectiveness
- **Type of Need**: Success criteria
- **Associated with Pain?**: No - quality benchmark
- **Priority**: Target goal

### 5. Demographic Segmentation Capability
- **Need Statement**: Screen respondents and capture demographics for analysis
- **Context**: Understanding who provided which responses
- **Type of Need**: Enabling capability
- **Associated with Pain?**: Yes - prevents watered-down results from unqualified respondents
- **Success Criteria**: Ability to filter and segment responses by respondent characteristics

## Aspirations & Opportunities

### 1. Present Network State During Conference
- **Aspiration Statement**: "I would want to present. The state of our network. Internally on stage and say, all right, thanks for fill out the survey… Let's see how our networks do."
- **Inspiration Source**: Employee survey model from corporate annual meetings
- **Why It Excites Them**: Creates cohesive understanding and sparks conversation about network health
- **Exploration vs Commitment**: Ready to commit - suggests specific implementation ideas
- **Quote**: "It's like an employee survey, right? When you go to the yearly meeting for your company… You want to see all that stuff, right?"

### 2. Leverage Client Intelligence for Business Development
- **Aspiration Statement**: Use survey data to identify client opportunities and demonstrate untapped value
- **Inspiration Source**: Understanding network capabilities and client limitations
- **Why It Excites Them**: "When I walk into a client someday… I want to be able to say, listen… you're leaving money on the table"
- **Exploration vs Commitment**: High priority for client partner role
- **Quote**: "It's not going to cost you any more for us to use AI… But your artificially holding yourself back from innovation that we could be giving you."

### 3. Pre-Conference Network Assessment
- **Aspiration Statement**: Survey network before conference and present results during event
- **Inspiration Source**: Making survey data part of conference content
- **Why It Excites Them**: Engages network in understanding their collective state
- **Exploration vs Commitment**: Interested and wants to explore implementation

## Current Workflow

### Forrester Survey Creation and Execution Workflow

**Trigger**: Need for market research data (thought leadership, total economic impact, wave evaluation)

**Goal**: Generate statistically significant, actionable market insights

**Steps**:

1. **Strategy Definition** - Determine survey objectives and how results will be used
   - Actions: Define research goals and intended use of data
   - Tools/systems: Internal strategy sessions
   - Decision points: What conclusions do we need to draw?

2. **Question Development** - Data researcher helps curate questions based on desired outcomes
   - Actions: Collaborate with researchers to design questions
   - Tools/systems: Data researcher expertise
   - Decision points: Do questions align with strategy?

3. **Demographic Screening** - Create screener elements to filter for qualified respondents
   - Actions: Design demographic questions to target right audience
   - Tools/systems: Survey platform screening logic
   - Decision points: Who is qualified to answer each question?

4. **Survey Construction** - Include various question types (scales, multiple choice, open-ended)
   - Actions: Build survey in platform
   - Tools/systems: Qualtrics (for waves), other survey tools
   - Decision points: Question types and flow

5. **Third-Party Distribution** - Use anonymous company to send surveys to qualified demographics
   - Actions: Contract with distribution partner
   - Tools/systems: Third-party survey distribution company
   - Decision points: Target demographics and sample size

6. **Response Collection** - Minimum 50 responses required for statistical significance
   - Actions: Monitor response rates
   - Tools/systems: Survey platform analytics
   - Success criteria: 50+ responses per question

7. **Data Analysis** - Receive anonymized results and determine statistical significance
   - Actions: Analyze data for trends and patterns
   - Tools/systems: Excel, statistical analysis tools
   - Decision points: Is data statistically significant?

8. **Results Interpretation** - Create insights and conclusions from data
   - Actions: Develop slides and reports
   - Tools/systems: PowerPoint, data visualization
   - Success criteria: Actionable insights generated

**Tools Used**: Data researchers, Qualtrics (for waves), third-party distribution companies

**Pain Points in Workflow**: Questions missing intended outcomes, untested survey logic, committee-designed surveys lacking focus

**What's Missing**: Better upfront strategy alignment, thorough testing protocols

## Feature Reactions

### Granola Recording and Note-Taking Tool
- **Initial Reaction**: Positive interest and curiosity
- **Would they use it?**: Yes, expressed interest
- **Rationale**: Likes the combination of simultaneous note-taking with transcription and template flexibility
- **Quote**: "Oh, that's kind of cool… you can switch templates and it'll completely change how it interprets the meeting"

### Mandatory Survey Before Departure
- **Initial Reaction**: Supportive with practical suggestions
- **Would they use it?**: Yes, prefers this approach
- **Rationale**: Avoids travel timing conflicts and ensures completion while experience is fresh
- **Suggested Improvements**: "If someone says, you know, I forgot to fill the survey, give them a drink ticket anyway, right? You don't want to piss anybody off"

## Memorable Quotes

> "Sometimes the questions that you asked or the options that you gave didn't drive the results that you actually wanted. Like you got interesting information, but, like, that's really not answering the question."

> "If equal Experts took any actions based on it… If they don't take any action about it. If nothing changes. Then I'm going to say, then why? Why did I take that survey?"

> "When I walk into a client someday… I want to be able to say, listen… you're leaving money on the table. It's not going to cost you any more for us to use AI… But your artificially holding yourself back from innovation that we could be giving you."

> "Having a strategy for the survey is the most important thing and the most regretful thing in the past."

> "At the end of the day, the survey doesn't do you any good if it doesn't allow you to create conclusions about where things are going or what people's trends are, what their preferences are."

## Behavioral Observations

- **Research-oriented approach**: Consistently thinks about data quality, statistical significance, and methodology
- **Practical problem-solver**: Offers specific, actionable suggestions based on experience
- **Client-focused mindset**: Frames survey opportunities in terms of client value and business development
- **Quality-focused**: Emphasizes testing, proper design, and meaningful outcomes over quick execution
- **Analytical thinking**: Naturally breaks down complex processes into components and considers multiple variables
- **Results-driven**: Consistently returns to theme of surveys needing to drive action and change

## Follow-up Questions

1. What specific client opportunities have you identified that could be captured through network surveys?
2. How would you structure the demographic questions to best segment Equal Experts network responses?
3. What are the top 3 questions you would want to ask the network about their client experiences?
4. How should Equal Experts measure and track actions taken based on survey feedback?
5. What would be the ideal format and timing for presenting network survey results at future conferences?
6. How could survey data be integrated into client conversations most effectively?
7. What other survey tools or approaches have you seen that might work better than traditional forms?

## Next Steps

- Test survey functionality thoroughly before deployment to avoid missing questions
- Consider implementing survey requirement before conference departure rather than post-event distribution
- Design survey strategy first, then work backwards to question design
- Plan to present survey results (from pre-conference network survey) during conference
- Include both raw data export and interpreted analysis in final deliverables
